# Overview
We currently record candidate sessions by capturing DOM mutations and screenshots for integrity checks. However, this approach has led to performance issues across project questions.
To address this, we propose introducing full screen recording, which would:
- Improve session replay performance and provide a complete view for recruiters.
- Enable more comprehensive integrity verification using video streams.
- Allow consolidation of all integrity-related checks under a central service for easier management and scalability.

## Objectives
- **Records and streams the candidate’s screen** to HackerRank downstream services in a **highly performant** manner.
- **Supports near real-time integrity analysis** — frames should be consumable as soon as they are available, enabling integrity checks to complete within minutes after an attempt ends.
- **Provides a seamless playback experience** through a compatible player supporting:
    - `PLAY`
    - `SEEK_FORWARD_{SPEED}`
    - `SEEK_BACKWARD_{SPEED}`
    - Rendering of integrity events on the timeline.
- **Focuses initially on storage only** — recordings will be stored (e.g., on S3) without immediate analysis. During this phase, both **screenshots and screen recordings** will be uploaded for continuity.

## Engineering
There are two ways to approach this problem, either we create/augment integrity-events-service to be a full fledged proctoring + playback service or we have two independent services which will be responsible for their individual tasks - one for proctoring and one for playback. 

### IES as Full fledged Proctoring + Recording solution
- This approach modifies the existing IES service as proctoring service.
- It'll be responsible for managing the session recording -> generating playlist, processing video/image streams etc. 
- It'll also be responsible for doing all proctoring related things, webcam analysis, screenshot analysis, inserting proctoring events in cassandra etc. 
- Currently, IES, though decoupled, has no concept of a session. We interact with the API's via `attempt_type, attempt_id`. We'll have to introduce a new set of v2 APIs that provide a session object against which we'll interact with. A new DB (mySQL) would be required here.

### IES as proctoring + Session Replay as recorder solution
- This approach keeps the recording aspect decoupled from proctoring approach. Any service (including IES) can consume assets generated by session replay service as part of their workflow. 
- SRS will have the following responsibilities
	- asset upload of webcam/screenshare (possibly dual camera feed in case we wanna make it)
	- stitch the video feed, fill in the missing videos data with placeholder images.
	- create consumable HLS playlist
	- generate thumbnails
	- take care of achievable + deletion policies
	- Send further events so other services can take on the responsibilities
- Requires the least changes on our current infrastructure while having the modularity to add more features on top of it. 

In this document, we'll deep dive into approach 1 as we try to consolidate IES as a full fledged proctoring solution.
# Architecture
Proctoring service will be a standalone service responsible for all proctoring needs (session recording, running analysis etc) and hackerrank will act as one of its customers.

For the very initial version of proctoring service, we'll be designing the following flows from perspective of HRW
- **Onboarding Flow** - HRW will first register itself as a customer for Proctoring Service. This handoff will first include registering hackerrank as a customer for proctoring service. It'll issue an `api_key` 
- **Lifecycle Flows** - Henceforth, HRW will be solely responsible for calling endpoints to initialize a proctoring session and ending a proctoring session and fetching results via API endpoints and proxying other necessary endpoints as well. 

Now under product flows, We have three broad flows to engineer in our new proctoring service
- **Session Recording**: We need backend to provide support for enabling screen recording on frontend side. 
- **Session Processing**: We need to process the session recording (creating a playable link, extracting screenshots for ss analysis, creating thumbnails etc). This will also extend to webcam recordings.
- **Session Playback**: We need to be able to playback the session while having support for the previous version of session recorder. 

Every session will be represented by a unique `session_uuid` on the integrity events service. We'll need to create a new MYSQL DB on integrity-events-service for facilitating this and storing (product_type, product_id) mapping vs a unique `session_uuid`.
## File Storage
```
# RAW Files
/proctor/ies/<tenant_uuid>/<session_uuid>/<asset_name>/raw/
	- [Individual webM chunks]

# Processed Files
/proctor/ies/<tenant_uuid>/<session_uuid>/<asset_name>/processed/
	- hls_playlist
	 ├── master.m3u8
	 ├── index_360p.m3u8
	 ├── index_480p.m3u8
	 ├── index_720p.m3u8
	 ├── 720p/
	 ├── 360p/
	 ├── 480p/
	- frames
	- frame_thumbnails
	- stitched_video
	- playlist_cache
	  
# Reports
/proctor/ies/<tenant_uuid>/<session_uuid>/reports/
	- screenshot_analysis/
	- webcam_analysis/
	- audio_analysis/
```

## MySQL Storage
```sql
-- Tenants Table
CREATE TABLE tenants (
    uuid CHAR(36) PRIMARY KEY,
    slug VARCHAR(255) NOT NULL UNIQUE,
    name VARCHAR(255) NOT NULL,
    status ENUM('active', 'inactive', 'suspended') DEFAULT 'active',
    config JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_tenants_slug (slug)
)

-- Sessions Table
CREATE TABLE sessions (
    uuid CHAR(36) PRIMARY KEY,
    tenant_uuid CHAR(36) NOT NULL,
    unique_identifier VARCHAR(255) NOT NULL,
    session_metadata JSON, -- holds ip, agent, location, etc.
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    FOREIGN KEY (tenant_uuid) REFERENCES tenants(uuid) ON DELETE CASCADE,
    UNIQUE KEY uniq_tenant_identifier (tenant_uuid, unique_identifier),
)

-- API Keys Table
CREATE TABLE api_keys (
    uuid CHAR(36) PRIMARY KEY,
    tenant_uuid CHAR(36) NOT NULL,
    key_hash CHAR(64) NOT NULL UNIQUE,
    label VARCHAR(255),
    scopes JSON,
    last_used_at DATETIME NULL,
    expires_at DATETIME NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

    FOREIGN KEY (tenant_uuid) REFERENCES tenants(uuid) ON DELETE CASCADE,
    INDEX idx_api_keys_tenant_uuid (tenant_uuid)
)
```

Let's deep dive into the individual layers 
### Session Recording
![[Integrity Session Recording]]

### Processing
- We'll utilize a single SQS queue to trigger backend processing of the session.
- For initial version, we dont need a kafka or event bus as we'll only be storing the raw videos and not doing any significant post processing on this.

![[Integrity Session Processing]]

### Playback
![[Integrity Session Playback]]

# Backend Contracts

## Integrity Events Service
We'll be creating new session entries

### 1. POST /ies/api/v2/session
Creates a session on service side. These APIs will be called from HRW
#### Payload
```json
// HEADERS
// X-API-TOKEN: <api_key>

{
	unique_identifier: <string>, // unique identifier per tenant
	session_metadata: <json>,
}
```
#### Response
```json
{
	session_id: <uuid>
}
```

### 2. GET /ies/api/v2/session
Returns the session details
#### Payload
```json
// HEADERS
// X-API-TOKEN: <api_key>
{
	success: <bool>,
	session_id: <uuid>
}
```
#### Response
```json
{
	success: <bool>,
	playback: {
		url: <string>,
		type: <enum> // fullvideo, hls_masterplaylist
	}
}
```

### 3. GET /ies/api/v2/session/presigned-policy
Returns the signed policy for facilitating frontend s3 upload. Supports fetch by both (session_id) and (product_type, product_id)
#### Payload
```json
// HEADERS
// X-API-TOKEN: <api_key>
{
	session_id: <uuid>
}
```
#### Response
```json
{
	"postFields": {
		"Content-Type": "image/jpg",
		"acl": "private",
		"key":"proctor/screenshare_stream/screen_attempt/test1280x720/raw/",
		"policy": "eyJjb2..",
		"x-amz-algorithm": "AWS4-HMAC-SHA256",
		"x-amz-credential": "ASIAY/20251117/us-east-1/s3/aws4_request",
		"x-amz-date": "20251117T064827Z",
		"x-amz-security-token": "IQoJb3JpZ2lu...",
		"x-amz-signature": "cf396668906e50e893e416b...e"
	},
	"url": "https://hr-preprod-istreet-proctor.s3.us-east-1.amazonaws.com"
}
```

### 4. POST /ies/api/v2/session/process
Starts processing of the session 
#### Payload
```json
// HEADERS
// X-API-TOKEN: <api_key>
{
	session_id: <uuid>
}
```
#### Response
`HEAD OK`

### SRS SQS Consumer
- This consumer will listen to the SQS and do simple post-processing of the video.
- This consumer is also responsible for creating the HLS playlist. Since, for now, we'll be doing this post the test is ended, this consumer can only do HLS playlist generation. 
- For screen-recordings that are missing chunks, we’ll replace them with a placeholder segment video (black screen) to not break continuity.
## Technical Considerations

- We need to sync the user’s clock with server clock as part of system check screens. We saw one ticket wrt user setting his clock something else from standard internet time. This can mess up screen-recording timestamps if user changes his clock midway during the test.
- If we want to be able to read in almost realtime for integrity events analysis, we’ll have to ensure the videos we upload to s3 are self contained webm files. If we upload the continuous streams, we can only playback the video once the attempt has ended or we’ll have to continuously keep replaying the video stream from the beginning.
