{
	"nodes":[
		{"id":"cf74734d5685b2b2","type":"text","text":"# Event Streaming\n- Its the practice of capturing data in real time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events.\n- This data often needs to be stored durably for some time, consumed by other downstream services for processing and reacting to event streams.\n","x":-380,"y":-420,"width":360,"height":340},
		{"id":"6785f0fe2d9e80f3","type":"text","text":"# Apache Kafka\nIt's an event streaming platform. It implements 3 key capabilities so we can implement our use case for event streaming. \n\n- To Publish (write) and subscribe to (read) streams of events. \n- To store streams of events durably and reliably for as long as you want and play it back.\n- To process streams of events as they occur or retrospectively \n\nAll this is provided in a distributed, highly scalable, elastic, faul tolerant and secure manner. ","x":120,"y":-440,"width":340,"height":460},
		{"id":"bcc7ffea89450eaa","type":"text","text":"# The filesystem\n- A typical HDD filesystem will perform linear writes at about 600 MBps while it performs random writes at about 100 Kbps, over 6000 times slower than linear writes\n- Linear reads and writes are often predictable and are heavily optimised by the operating system. \n-  kafka’s design ensures:\n    - every write is **append-only**.\n    - every read is **mostly sequential**.\n    - the kernel does caching/buffering for you.\n    so kafka’s brokers don’t try to manage memory manually; they **delegate that to the OS page cache** and lean on modern file system semantics.\n## Structure\neach topic partition is a **directory on disk** like:\n```\n/var/lib/kafka/data/\n└── my-topic-0/\n    ├── 00000000000000000000.log\n    ├── 00000000000000000000.index\n    ├── 00000000000000000000.timeindex\n    ├── 00000000000000000000.txnindex\n    ├── 00000000000012345678.log\n    ├── 00000000000012345678.index\n    └── ...\n```\n### Segment File\n- kafka splits each partition into **segments**.\n- a segment is a chunk of contiguous messages, typically ~1 GB.\n- each segment is identified by its **base offset** — the first message’s offset in that file.\n- so `00000000000012345678.log` contains messages starting at offset 12,345,678.\n- segmenting makes truncation, retention, and compaction efficient: kafka can delete or rewrite entire files without touching other data.\n### Indexes\nEach segment has two auxillary files:","x":520,"y":-1240,"width":480,"height":700}
	],
	"edges":[
		{"id":"b0da778b2b7f133c","fromNode":"cf74734d5685b2b2","fromSide":"right","toNode":"6785f0fe2d9e80f3","toSide":"left"},
		{"id":"74fbaf5c20a1cf78","fromNode":"6785f0fe2d9e80f3","fromSide":"top","toNode":"bcc7ffea89450eaa","toSide":"left"}
	]
}